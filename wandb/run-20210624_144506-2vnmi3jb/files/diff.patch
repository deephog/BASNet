diff --git a/.idea/BASNet.iml b/.idea/BASNet.iml
index d0876a7..880c2e3 100644
--- a/.idea/BASNet.iml
+++ b/.idea/BASNet.iml
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="inheritedJdk" />
+    <orderEntry type="jdk" jdkName="Python 3.8 (torch)" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
 </module>
\ No newline at end of file
diff --git a/.idea/misc.xml b/.idea/misc.xml
index 1f3dad7..c37d577 100644
--- a/.idea/misc.xml
+++ b/.idea/misc.xml
@@ -1,4 +1,4 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (BASNet)" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.8 (torch)" project-jdk-type="Python SDK" />
 </project>
\ No newline at end of file
diff --git a/after_resize.png b/after_resize.png
deleted file mode 100644
index 5c89bf5..0000000
Binary files a/after_resize.png and /dev/null differ
diff --git a/basnet_test.py b/basnet_test.py
index 6a426e1..874302d 100755
--- a/basnet_test.py
+++ b/basnet_test.py
@@ -7,17 +7,20 @@ import torch.nn as nn
 import torch.nn.functional as F
 from torch.utils.data import Dataset, DataLoader
 from torchvision import transforms#, utils
+import cv2
 # import torch.optim as optim
-
+from data import test_dataset
 import numpy as np
+import onnx
 from PIL import Image
 import glob
 import time
+from data import get_loader
 
 from data_loader import RescaleT
 from data_loader import CenterCrop
 from data_loader import ToTensor
-from data_loader import ToTensorLab
+from data_loader import ToTensorLab, OtherTrans
 from data_loader import SalObjDataset
 
 from model import BASNet
@@ -26,25 +29,33 @@ os.environ["CUDA_VISIBLE_DEVICES"] = '1'
 def normPRED(d):
 	ma = torch.max(d)
 	mi = torch.min(d)
-
 	dn = (d-mi)/(ma-mi)
-
 	return dn
 
-def save_output(image_name,pred,d_dir):
+def overlay(image, mask):
+	mask_3 = np.tile(np.expand_dims(mask, axis=-1), (1, 1, 3))
+	olay = np.multiply(image, mask_3)
+	return olay
 
-	predict = pred
-	predict = predict.squeeze()
-	predict_np = predict.cpu().data.numpy()
-	
-	io.imsave('inter_img.png', predict_np*255)
 
-	im = Image.fromarray(predict_np*255).convert('RGB')
+def save_output(image_name, pred, d_dir, o_dir):
+	pred = pred.squeeze()
+	pred = pred.cpu().data.numpy()
+	th = 0.1
+	pred[pred > th] = 1
+	pred[pred <= th] = 0
+
 	img_name = image_name.split("/")[-1]
 	image = io.imread(image_name)
-	imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)
 
-	pb_np = np.array(imo)
+	mask = transform.resize(pred, (image.shape[0],image.shape[1]), anti_aliasing=False, mode = 'constant', order=0)
+	mask = np.tile(np.expand_dims(mask, axis=-1), (1, 1, 3))
+	#kernel = np.ones((3, 3), np.uint8)
+	#mask = cv2.erode(mask, kernel, iterations=4)
+	olay = image * mask
+
+
+	#pb_np = np.array(imo)
 
 	aaa = img_name.split(".")
 	bbb = aaa[0:-1]
@@ -52,55 +63,59 @@ def save_output(image_name,pred,d_dir):
 	for i in range(1,len(bbb)):
 		imidx = imidx + "." + bbb[i]
 
-	imo.save(d_dir+imidx+'.png')
+	io.imsave(o_dir+imidx+'.jpg', olay)
+	io.imsave(d_dir + imidx + '.jpg', mask)
 
 
 if __name__ == '__main__':
 	# --------- 1. get image path and name ---------
 	
-	image_dir = './test_data/test_images/'
-	prediction_dir = './test_data/test_results/'
-	model_dir = './saved_models/basnet_bsi_human/basnet_441.pth'
+	image_dir = '/home/hypevr/Desktop/data_0616/xy/2/image/'#'/media/hypevr/KEY/tonaci_selected/'#'./test_data/test_images/'
+	prediction_dir = '/home/hypevr/Desktop/data_0616/xy/2/mask/'#'/media/hypev/KEY/tonaci_selected_masks/'
+	olay_dir = '/home/hypevr/Desktop/data_0616/xy/2/olay/'#'/media/hypevr/KEY/tonaci_selected_olay/'
+	model_dir = './saved_models/basnet_bsi_human2_fr0.2_pb_0.2/basnet_209.pth' #refine/
+	plate_dir = '/home/hypevr/Desktop/data_0616/xy/2/back'
 	
 	img_name_list = glob.glob(image_dir + '*.jpg')
 	
 	# --------- 2. dataloader ---------
 	#1. dataload
-	test_salobj_dataset = SalObjDataset(img_name_list = img_name_list, lbl_name_list = [],transform=transforms.Compose([RescaleT(256),ToTensorLab(flag=0)]))
-	test_salobj_dataloader = DataLoader(test_salobj_dataset, batch_size=1,shuffle=False,num_workers=1)
-	
+	##test_salobj_dataset = SalObjDataset(img_name_list = img_name_list, lbl_name_list = [],transform=transforms.Compose([RescaleT(352), ToTensorLab(flag=0)])) #,OtherTrans()
+	#test_salobj_dataloader = DataLoader(test_salobj_dataset, batch_size=1,shuffle=False,num_workers=1)
+	# test_salobj_dataloader = get_loader(image_dir, prediction_dir, batchsize=1,
+	# 								  trainsize=416)
+	test_loader = test_dataset(image_dir, image_dir, 352, True)
 	# --------- 3. model define ---------
 	print("...load BASNet...")
-	net = BASNet(3,1)
-	net.load_state_dict(torch.load(model_dir))
-	if torch.cuda.is_available():
-		net.cuda()
+	net = BASNet(3, 1)
+	#net = nn.DataParallel(net)
+	net.load_state_dict(torch.load(model_dir)) #, map_location='cuda:0'
+	net.cuda()
 	net.eval()
 	scriptedmodel = torch.jit.script(net)
-	torch.jit.save(scriptedmodel, 'scripted_BASNet_d2.pt')
+	torch.jit.save(scriptedmodel, 'scripted_BASNet_57.pt')
+	x = torch.ones((4, 3, 320, 320)).cuda()
+	torch.onnx.export(net, x, "basnet.onnx", opset_version=11)
+	onnx_model = onnx.load("basnet.onnx")
+	onnx.checker.check_model(onnx_model)
+	#net.eval()
 	
 	#example = torch.rand(1, 3, 256, 256).cuda()
 	#traced_script_module = torch.jit.trace(net, example)
 	#traced_script_module.save("traced_model_BASNet.pt")
-	net = torch.load('scripted_BASNet_d2.pt')
+	net = torch.load('scripted_BASNet_57.pt')
 	net.eval()
 	
 	# --------- 4. inference for each image ---------
-	for i_test, data_test in enumerate(test_salobj_dataloader):
-	
-		#print("inferencing:",img_name_list[i_test].split("/")[-1])
-		
-		#print(data_test['image'].shape)
-		
-		#img = data_test['image']*256.
-		#io.imsave('test_image.png', img)
-	
-		inputs_test = data_test['image']
+	for i in range(test_loader.size):
+		image_orig, inputs_test, gt, name = test_loader.load_data()
+		##inputs_test = data_test[0]
 		
 		inputs_test = inputs_test.type(torch.FloatTensor)
-		
-		
-		io.imsave('after_resize.png', inputs_test.numpy()[0, :, :, :].transpose((1, 2, 0)))
+
+		image_resized = inputs_test.numpy()[0, :, :, :].transpose((1, 2, 0))
+
+		#io.imsave('after_resize.png', inputs_test.numpy()[0, :, :, :].transpose((1, 2, 0)))
 	
 		if torch.cuda.is_available():
 			inputs_test = Variable(inputs_test.cuda())
@@ -108,22 +123,16 @@ if __name__ == '__main__':
 			inputs_test = Variable(inputs_test)
 	
 		start = time.time()
-		d1 = net(inputs_test)#,d2,d3,d4,d5,d6,d7,d8
+		d1 = net(inputs_test)#,d2,d3,d4,d5,d6,d7,d8 ,, d2, d3, d4, d5, d6, d7, d8
 		#print(d1)
 		torch.cuda.synchronize()
 		#d1 = d1.cpu() 
 		print(time.time()-start)
-		
-		print(d1.shape)
-		#traced_script_module = torch.jit.trace(net, inputs_test)
-		#traced_script_module.save("traced_model_BASNet.pt")
-	
-		# normalization
-		#d1 = torch.nn.functional.sigmoid(d1)
-		#pred = d1[:,0,:,:]
+
+
 		pred = normPRED(d1)
-	
+		#pred = overlay(image_resized, pred.squeeze().cpu().data.numpy())
 		# save results to test_results folder
-		save_output(img_name_list[i_test],pred,prediction_dir)
+		save_output(image_dir+name, pred,prediction_dir, olay_dir)
 	
 		#del d1,d2,d3,d4,d5,d6,d7,d8
diff --git a/basnet_train.py b/basnet_train.py
index acc4e3f..52658fe 100644
--- a/basnet_train.py
+++ b/basnet_train.py
@@ -7,8 +7,8 @@ import torch.nn.functional as F
 from torch.utils.data import Dataset, DataLoader
 from torchvision import transforms, utils
 import torch.optim as optim
+import wandb
 import torchvision.transforms as standard_transforms
-
 import numpy as np
 import sys
 import glob
@@ -21,6 +21,7 @@ from data_loader import CenterCrop
 from data_loader import ToTensor
 from data_loader import ToTensorLab
 from data_loader import SalObjDataset
+from data import get_loader
 
 from skimage import io
 
@@ -29,13 +30,41 @@ from model import BASNet
 import pytorch_ssim
 import pytorch_iou
 import os
-os.environ["CUDA_VISIBLE_DEVICES"] = '1'
+
 # ------- 1. define loss function --------
 
 bce_loss = nn.BCELoss(size_average=True)
 ssim_loss = pytorch_ssim.SSIM(window_size=11,size_average=True)
 iou_loss = pytorch_iou.IOU(size_average=True)
 
+hyperparameter_defaults = {
+              "gpu": '0, 1',
+              "learning_rate": 1e-4,
+              "lr_decay": 0,
+              "epochs": 1000,
+              "batch_size": 8,
+              "checkpoint": 213,
+              "load_pretrained": False,
+              "trainsize": 352,
+              "fb_rate": 0.1,
+              "ob_rate": 0.1,
+              "with_plate": True,
+              "plate_dir": '',
+              "model_dir": "./saved_models/basnet_bsi_human2_fr0.2_pb_0.2/"
+}
+
+
+run = wandb.init(project='basnet_refine', config=hyperparameter_defaults, save_code='on', mode='online', reinit=True)
+config = run.config
+os.environ["CUDA_VISIBLE_DEVICES"] = config.gpu
+if len(config.gpu.split(',')) > 1:
+    multi_gpu = True
+else:
+    multi_gpu = False
+
+if not os.path.isdir(config.model_dir):
+    os.makedirs(config.model_dir)
+
 def bce_ssim_loss(pred,target):
 
     bce_out = bce_loss(pred,target)
@@ -46,35 +75,35 @@ def bce_ssim_loss(pred,target):
 
     return loss
 
-def muti_bce_loss_fusion(d1, labels_v)#d2, d3, d4, d5, d6, labels_v):
+def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v):
 
-    #loss0 = bce_ssim_loss(d0,labels_v)
+    loss0 = bce_ssim_loss(d0,labels_v)
     loss1 = bce_ssim_loss(d1,labels_v)
-    # loss2 = bce_ssim_loss(d2,labels_v)
-    # loss3 = bce_ssim_loss(d3,labels_v)
-    # loss4 = bce_ssim_loss(d4,labels_v)
-    # loss5 = bce_ssim_loss(d5,labels_v)
-    # loss6 = bce_ssim_loss(d6,labels_v)
-    #loss7 = bce_ssim_loss(d7,labels_v)
+    loss2 = bce_ssim_loss(d2,labels_v)
+    loss3 = bce_ssim_loss(d3,labels_v)
+    loss4 = bce_ssim_loss(d4,labels_v)
+    loss5 = bce_ssim_loss(d5,labels_v)
+    loss6 = bce_ssim_loss(d6,labels_v)
+    loss7 = bce_ssim_loss(d7,labels_v)
     #ssim0 = 1 - ssim_loss(d0,labels_v)
 
     # iou0 = iou_loss(d0,labels_v)
     #loss = torch.pow(torch.mean(torch.abs(labels_v-d0)),2)*(5.0*loss0 + loss1 + loss2 + loss3 + loss4 + loss5) #+ 5.0*lossa
-    #loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6#+ 5.0*lossa
+    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7#+ 5.0*lossa
     # print("l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\n"%(loss0.data[0],loss1.data[0],loss2.data[0],loss3.data[0],loss4.data[0],loss5.data[0],loss6.data[0]))
     # print("BCE: l1:%3f, l2:%3f, l3:%3f, l4:%3f, l5:%3f, la:%3f, all:%3f\n"%(loss1.data[0],loss2.data[0],loss3.data[0],loss4.data[0],loss5.data[0],lossa.data[0],loss.data[0]))
     #print("\r l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f" % (loss1.item(), loss2.item(), loss3.item(), loss4.item(), loss5.item(), loss6.item()))
-    return loss1#, loss
+    return loss0, loss
 
 
 # ------- 2. set the directory of training dataset --------
 
-data_dir = './train_data/'
-tra_image_dir = 'HUMAN/train/image/'
-tra_label_dir = 'HUMAN/train/mask/'
+data_dir = '/home/hypevr/Desktop/data/projects/data/human2/'
+tra_image_dir = 'train/image/'
+tra_label_dir = 'train/mask/'
 
-te_image_dir = 'HUMAN/val/image/'
-te_label_dir = 'HUMAN/val/mask/'
+te_image_dir = 'val/image/'
+te_label_dir = 'val/mask/'
 
 # tra_image_dir = 'dummy_img/'
 # tra_label_dir = 'dummy_gt/'
@@ -83,22 +112,24 @@ te_label_dir = 'HUMAN/val/mask/'
 # te_label_dir = 'dummy_gt/'
 
 image_ext = '.jpg'
-label_ext = '.png'
-
-model_dir = "./saved_models/basnet_bsi_human_d1/"
+label_ext = '.jpg'
 
+model_dir = config.model_dir
 
 ##############################
-checkpoint = None
+checkpoint = config.checkpoint
+load_pretrained = config.load_pretrained
 #############################
 
-
 if checkpoint:
-    checkpoint_dir = model_dir + 'basnet_' + str(451) + '.pth'
+    checkpoint_dir = model_dir + 'basnet_' + str(checkpoint) + '.pth'
 
-epoch_num = 1000
-batch_size_train = 32
-batch_size_val = 32
+if load_pretrained:
+    checkpoint_dir = './saved_models/basnet_bsi/basnet.pth'
+
+epoch_num = config.epochs
+batch_size_train = config.batch_size
+batch_size_val = config.batch_size
 train_num = 0
 val_num = 0
 
@@ -107,6 +138,8 @@ te_img_name_list = glob.glob(data_dir + te_image_dir + '*' + image_ext)
 
 tra_lbl_name_list = []
 te_lbl_name_list = []
+
+
 for img_path in tra_img_name_list:
     img_name = img_path.split("/")[-1]
 
@@ -140,38 +173,45 @@ train_num = len(tra_img_name_list)
 val_num = len(te_img_name_list)
 
 
-salobj_dataset = SalObjDataset(
-    img_name_list=tra_img_name_list,
-    lbl_name_list=tra_lbl_name_list,
-    img_transform=transforms.Compose([OtherTrans()]),
-    transform=transforms.Compose([
-        RescaleT(256),
-        RandomCrop(224),
-        ToTensorLab(flag=0),
-    ]))
-
-salobj_dataset_te = SalObjDataset(
-    img_name_list=te_img_name_list,
-    lbl_name_list=te_lbl_name_list,
-    transform=transforms.Compose([
-        RescaleT(224),
-        #RandomCrop(224),
-        ToTensorLab(flag=0),
-    ]))
-salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)
-salobj_dataloader_te = DataLoader(salobj_dataset_te, batch_size=1, shuffle=False, num_workers=1)
+# salobj_dataset = SalObjDataset(
+#     img_name_list=tra_img_name_list,
+#     lbl_name_list=tra_lbl_name_list,
+#     img_transform=transforms.Compose([OtherTrans()]),
+#     transform=transforms.Compose([
+#         RescaleT(512),
+#         RandomCrop(352),
+#         ToTensorLab(flag=0),
+#     ]))
+#
+# salobj_dataset_te = SalObjDataset(
+#     img_name_list=te_img_name_list,
+#     lbl_name_list=te_lbl_name_list,
+#     transform=transforms.Compose([
+#         RescaleT(352),
+#         #RandomCrop(224),
+#         ToTensorLab(flag=0),
+#     ]))
+#salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)
+#salobj_dataloader_te = DataLoader(salobj_dataset_te, batch_size=1, shuffle=False, num_workers=1)
+back_dir = '/home/hypevr/Desktop/data/projects/background/image/'
+
+salobj_dataloader = get_loader(data_dir+tra_image_dir, data_dir+tra_label_dir, batchsize=config.batch_size, trainsize=config.trainsize, fake_back_rate=config.fb_rate, back_dir=back_dir, pure_back_rate=config.ob_rate)
+salobj_dataloader_te = get_loader(data_dir+te_image_dir, data_dir+te_label_dir, batchsize=config.batch_size, trainsize=config.trainsize, fake_back_rate=0, back_dir=None)
 
 # ------- 3. define model --------
 # define the net
 net = BASNet(3, 1)
+
 if torch.cuda.is_available():
     net.cuda()
 
-net.load_state_dict(torch.load(checkpoint_dir))
-
+if checkpoint or load_pretrained:
+    net.load_state_dict(torch.load(checkpoint_dir, map_location={'cuda:0':'cuda:1'}))
+net = nn.DataParallel(net)
+torch.cuda.empty_cache()
 # ------- 4. define optimizer --------
 print("---define optimizer...")
-optimizer = optim.Adam(net.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
+optimizer = optim.Adam(net.parameters(), lr=config.learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=config.lr_decay)
 
 # ------- 5. training process --------
 print("---start training...")
@@ -195,7 +235,7 @@ for epoch in range(0, epoch_num):
         ite_num = ite_num + 1
         ite_num4val = ite_num4val + 1
 
-        inputs, labels = data['image'], data['label']
+        inputs, labels = data
 
         # print(inputs.shape)
         #
@@ -217,15 +257,15 @@ for epoch in range(0, epoch_num):
         optimizer.zero_grad()
 
         # forward + backward + optimize
-        d0 = net(inputs_v)#, d1, d2, d3, d4, d5 =
-        loss = muti_bce_loss_fusion(d0, labels_v)#d1, d2, d3, d4, d5, labels_v) #loss2
+        d0, d1, d2, d3, d4, d5, d6, d7 = net(inputs_v)#
+        loss0, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v) #loss2
 
         loss.backward()
         optimizer.step()
 
         # # print statistics
         running_loss += loss.item()
-        running_tar_loss += loss.item()
+        running_tar_loss += loss0.item()
 
         # del temporary outputs and loss
         del d0, loss#d1, d2, d3, d4, d5, loss2, loss
@@ -234,9 +274,11 @@ for epoch in range(0, epoch_num):
             epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val,
             running_tar_loss / ite_num4val, time.time()-start_time))
 
+    wandb.log({'epochs': epoch,
+               'train_loss': float(running_tar_loss / ite_num4val),
+               })
 
-
-    if epoch % 10 == 1:  # save model every 2000 iterations
+    if epoch % 2 == 1:  # save model every 2000 iterations
         # basnet_bsi_itr_%d_train_%3f_tar_%3f.pth basnet_time.pth
         # torch.save(net.state_dict(), model_dir + "basnet_bsi_itr_%d_train_%3f_tar_%3f.pth" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))
 
@@ -249,7 +291,7 @@ for epoch in range(0, epoch_num):
                 sys.stdout.write("\033[F")
                 sys.stdout.write("\033[K")
             ind_v += 1
-            inputs, labels = data['image'], data['label']
+            inputs, labels = data
 
             inputs = inputs.type(torch.FloatTensor)
             labels = labels.type(torch.FloatTensor)
@@ -261,23 +303,30 @@ for epoch in range(0, epoch_num):
             else:
                 inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)
 
-            d0 = net(inputs_v) #, d1, d2, d3, d4, d5
-            loss = muti_bce_loss_fusion(d0, labels_v)#d1, d2, d3, d4, d5,
+            d0, d1, d2, d3, d4, d5, d6, d7 = net(inputs_v)#
+            loss0, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, d7, labels_v)#d1, d2, d3, d4, d5,
+
+            running_loss_v += loss.item()
+            running_tar_loss_v += loss0.item()
+
             print("(Validation Phase) [epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f " % (
                 epoch + 1, epoch_num, (i + 1) * batch_size_val, val_num, ind_v, running_loss_v / ind_v,
                 running_tar_loss_v / ind_v))
             # # print statistics
-            running_loss_v += loss.item()
-            running_tar_loss_v += loss.item()
 
             del d0, loss #d1, d2, d3, d4, d5, loss2,
 
-
-        torch.save(net.state_dict(), model_dir + "basnet_%d.pth" % (epoch))
+        if multi_gpu:
+            torch.save(net.module.state_dict(), model_dir + "basnet_%d.pth" % (epoch))
+        else:
+            torch.save(net.state_dict(), model_dir + "basnet_%d.pth" % (epoch))
         #running_loss = 0.0
         #running_tar_loss = 0.0
         net.train()  # resume train
         #ite_num4val = 0
+        wandb.log({
+                   'val_loss': float(running_tar_loss_v / ind_v)
+                   })
 
     # sys.stdout.write("\033[F")
     # sys.stdout.write("\033[F")
diff --git a/data_loader.py b/data_loader.py
index aad0617..d02b041 100755
--- a/data_loader.py
+++ b/data_loader.py
@@ -38,13 +38,7 @@ class RescaleT(object):
 		# lbl = transform.resize(label,(new_h,new_w),mode='constant', order=0, preserve_range=True)
 		#print(h, w)
 		img = transform.resize(image,(self.output_size, self.output_size),mode='constant', anti_aliasing=False)
-		#print('rescale')
-		#print(image[:, :, 0])
-		#print(img[:, :, 0])
-		#print(img[0, :, 0])
-		#print(img[:, 0, 0])
-		
-		
+
 		lbl = transform.resize(label,(self.output_size,self.output_size),mode='constant', order=0, preserve_range=True)
 
 		return {'image':img,'label':lbl}
@@ -261,7 +255,21 @@ class OtherTrans(object):
 
 	def __call__(self, sample):
 		image, label = sample['image'], sample['label']
-
+		print(image.shape)
+		print(np.unique(image))
+		w, h, c = image.shape
+		back = np.asarray(Image.open('back.jpg'))
+		back = transform.resize(back, (w, h), order=1, mode='constant')
+		#io.imsave('back.jpg', back)
+		#back = np.transpose(back, (2,0,1))
+		olay = image.copy()
+		#image_tran = np.transpose(image, (1, 2, 0))
+		#olay[image_tran[:, :] == (0, 0, 0)] = back
+		compare = np.all(image == (0, 0, 0), axis=-1)
+		olay[compare] = back[compare]
+
+		io.imsave('olay.jpg', olay)
+		input('wait')
 		#FLP = transforms.RandomHorizontalFlip()
 		#RPe = transforms.RandomPerspective(distortion_scale=0.1, p=0.5)
 		#RRo = transforms.RandomRotation(90)
diff --git a/data_split.py b/data_split.py
index 06a9c98..cf2164d 100644
--- a/data_split.py
+++ b/data_split.py
@@ -2,13 +2,13 @@ import os
 import random
 import shutil
 
-src_root = 'HUMAN/'
+src_root = '/home/hypevr/data/projects/data/human2/'
 path, dirs, files = next(os.walk(src_root + 'image/'))
 
 num_img = len(files)
 random.shuffle(files)
 
-train_pct = 0.8
+train_pct = 0.9
 
 train_list = files[:int(num_img*train_pct)]
 val_list = files[int(num_img*train_pct):]
@@ -16,9 +16,9 @@ val_list = files[int(num_img*train_pct):]
 for f in train_list:
     fname = f.split('.')[0]
     shutil.copy(src_root + 'image/' + f, src_root + 'train/image/' + f)
-    shutil.copy(src_root + 'mask/' + fname + '.png', src_root + 'train/mask/' + fname + '.png')
+    shutil.copy(src_root + 'mask/' + fname + '.jpg', src_root + 'train/mask/' + fname + '.jpg')
 
 for f in val_list:
     fname = f.split('.')[0]
     shutil.copy(src_root + 'image/' + f, src_root + 'val/image/' + f)
-    shutil.copy(src_root + 'mask/' + fname + '.png', src_root + 'val/mask/' + fname + '.png')
+    shutil.copy(src_root + 'mask/' + fname + '.jpg', src_root + 'val/mask/' + fname + '.jpg')
diff --git a/human_det.py b/human_det.py
index fb5ec33..c120edd 100644
--- a/human_det.py
+++ b/human_det.py
@@ -5,17 +5,22 @@ import os
 from shutil import copyfile
 
 
-data_fold = 'train_data/ECSSD/'
+data_fold = '/home/hypevr/Desktop/data/projects/data/DUTS/DUTS-TE/'
 src = data_fold+'image/'
-img_ext = 'jpg'
+img_ext = '.jpg'
 
 mask_src = 'mask/'
 mask_ext = '.png'
 
-dst_image = 'human_img/'
-dst_mask = 'human_mask/'
+dst_image = 'nonhuman_img/'
+dst_mask = 'nonhuman_mask/'
 
-mtcnn = MTCNN(image_size=160)
+if not os.path.isdir(data_fold + dst_mask):
+    os.mkdir(data_fold + dst_mask)
+if not os.path.isdir(data_fold + dst_image):
+    os.mkdir(data_fold + dst_image)
+
+mtcnn = MTCNN(image_size=300)
 path, dirs, files = next(os.walk(src))
 
 for f in files:
@@ -25,8 +30,9 @@ for f in files:
         if len(array_img.shape) > 2:
             detected, prob = mtcnn.detect(img)
             print(f, 'prob:', prob)
-            if prob[0]:
-                if max(prob) > 0.995:
-                    copyfile(src + f, data_fold + dst_image + f)
-                    copyfile(data_fold + mask_src + f.split('.')[0] + mask_ext, data_fold + dst_mask + f.split('.')[0] + mask_ext)
+            if not prob[0]:
+                #if max(prob) < 0.5:
+                copyfile(src + f, data_fold + dst_image + f)
+                copyfile(data_fold + mask_src + f.split('.')[0] + mask_ext, data_fold + dst_mask + f.split('.')[0] + mask_ext)
+            #else:
 
diff --git a/inter_img.png b/inter_img.png
deleted file mode 100644
index 087f397..0000000
Binary files a/inter_img.png and /dev/null differ
diff --git a/mask_check.py b/mask_check.py
new file mode 100644
index 0000000..ed2d33a
--- /dev/null
+++ b/mask_check.py
@@ -0,0 +1,25 @@
+import os
+from skimage import io, transform, color
+from skimage.viewer import ImageViewer
+import numpy as np
+
+src = 'train_data/HUMAN/train/mask/'
+path, dirs, files = next(os.walk(src))
+images = []
+for f in files:
+    image = io.imread(path+f, as_gray=True)
+    image = np.asarray(image)
+    viewer = ImageViewer(image)
+    viewer.show()
+    print(np.unique(image))
+
+    image[image<1.] = 0
+    viewer = ImageViewer(image)
+    viewer.show()
+    input('wait')
+    image = transform.resize(image, (256, 256), mode='constant', order=0, preserve_range=True)
+    images.append(image)
+
+images = np.asarray(images)
+print(np.unique(images))
+
diff --git a/model/BASNet.py b/model/BASNet.py
old mode 100755
new mode 100644
index a2155e3..8d6bdf8
--- a/model/BASNet.py
+++ b/model/BASNet.py
@@ -4,7 +4,6 @@ from torchvision import models
 import torch.nn.functional as F
 
 from .resnet_model import *
-from torchkeras import summary
 
 
 class RefUnet(nn.Module):
@@ -106,45 +105,41 @@ class BASNet(nn.Module):
     def __init__(self,n_channels,n_classes):
         super(BASNet,self).__init__()
 
-        resnet = models.mobilenet_v2(pretrained=True)
-        #print(summary(resnet, input_shape=(3, 224, 224)))
-        #print(resnet)
+        resnet = models.resnet34(pretrained=True)
 
         ## -------------Encoder--------------
 
-        #self.inconv = nn.Conv2d(n_channels,32,3,padding=1)
-        #self.inbn = nn.BatchNorm2d(32)
-        #self.inrelu = nn.ReLU(inplace=True)
+        self.inconv = nn.Conv2d(n_channels,64,3,padding=1)
+        self.inbn = nn.BatchNorm2d(64)
+        self.inrelu = nn.ReLU(inplace=True)
 
         #stage 1
-        self.encoder1 = resnet.features[:2] #112
+        self.encoder1 = resnet.layer1 #224
         #stage 2
-        self.encoder2 = resnet.features[2:4] #56
+        self.encoder2 = resnet.layer2 #112
         #stage 3
-        self.encoder3 = resnet.features[4:7] #28
+        self.encoder3 = resnet.layer3 #56
         #stage 4
-        self.encoder4 = resnet.features[7:14] #14
+        self.encoder4 = resnet.layer4 #28
+
+        self.pool4 = nn.MaxPool2d(2,2,ceil_mode=True)
+
         #stage 5
-        self.encoder5 = resnet.features[14:18] #7
-
-        # self.pool4 = nn.MaxPool2d(2,2,ceil_mode=True)
-        #
-        # #stage 5
-        # self.resb5_1 = BasicBlock(512,512)
-        # self.resb5_2 = BasicBlock(512,512)
-        # self.resb5_3 = BasicBlock(512,512) #14
-        #
-        # self.pool5 = nn.MaxPool2d(2,2,ceil_mode=True)
-        #
-        # #stage 6
-        # self.resb6_1 = BasicBlock(512,512)
-        # self.resb6_2 = BasicBlock(512,512)
-        # self.resb6_3 = BasicBlock(512,512) #7
+        self.resb5_1 = BasicBlock(512,512)
+        self.resb5_2 = BasicBlock(512,512)
+        self.resb5_3 = BasicBlock(512,512) #14
+
+        self.pool5 = nn.MaxPool2d(2,2,ceil_mode=True)
+
+        #stage 6
+        self.resb6_1 = BasicBlock(512,512)
+        self.resb6_2 = BasicBlock(512,512)
+        self.resb6_3 = BasicBlock(512,512) #7
 
         ## -------------Bridge--------------
 
         #stage Bridge
-        self.convbg_1 = nn.Conv2d(320,512,3,dilation=2, padding=2) # 7
+        self.convbg_1 = nn.Conv2d(512,512,3,dilation=2, padding=2) # 7
         self.bnbg_1 = nn.BatchNorm2d(512)
         self.relubg_1 = nn.ReLU(inplace=True)
         self.convbg_m = nn.Conv2d(512,512,3,dilation=2, padding=2)
@@ -157,7 +152,7 @@ class BASNet(nn.Module):
         ## -------------Decoder--------------
 
         #stage 6d
-        self.conv6d_1 = nn.Conv2d(832,512,3,padding=1) # 16
+        self.conv6d_1 = nn.Conv2d(1024,512,3,padding=1) # 16
         self.bn6d_1 = nn.BatchNorm2d(512)
         self.relu6d_1 = nn.ReLU(inplace=True)
 
@@ -170,7 +165,7 @@ class BASNet(nn.Module):
         self.relu6d_2 = nn.ReLU(inplace=True)
 
         #stage 5d
-        self.conv5d_1 = nn.Conv2d(608,512,3,padding=1) # 16
+        self.conv5d_1 = nn.Conv2d(1024,512,3,padding=1) # 16
         self.bn5d_1 = nn.BatchNorm2d(512)
         self.relu5d_1 = nn.ReLU(inplace=True)
 
@@ -183,7 +178,7 @@ class BASNet(nn.Module):
         self.relu5d_2 = nn.ReLU(inplace=True)
 
         #stage 4d
-        self.conv4d_1 = nn.Conv2d(544,512,3,padding=1) # 32
+        self.conv4d_1 = nn.Conv2d(1024,512,3,padding=1) # 32
         self.bn4d_1 = nn.BatchNorm2d(512)
         self.relu4d_1 = nn.ReLU(inplace=True)
 
@@ -196,7 +191,7 @@ class BASNet(nn.Module):
         self.relu4d_2 = nn.ReLU(inplace=True)
 
         #stage 3d
-        self.conv3d_1 = nn.Conv2d(280,256,3,padding=1) # 64
+        self.conv3d_1 = nn.Conv2d(512,256,3,padding=1) # 64
         self.bn3d_1 = nn.BatchNorm2d(256)
         self.relu3d_1 = nn.ReLU(inplace=True)
 
@@ -210,7 +205,7 @@ class BASNet(nn.Module):
 
         #stage 2d
 
-        self.conv2d_1 = nn.Conv2d(144,128,3,padding=1) # 128
+        self.conv2d_1 = nn.Conv2d(256,128,3,padding=1) # 128
         self.bn2d_1 = nn.BatchNorm2d(128)
         self.relu2d_1 = nn.ReLU(inplace=True)
 
@@ -223,24 +218,24 @@ class BASNet(nn.Module):
         self.relu2d_2 = nn.ReLU(inplace=True)
 
         #stage 1d
-        # self.conv1d_1 = nn.Conv2d(128,64,3,padding=1) # 256
-        # self.bn1d_1 = nn.BatchNorm2d(64)
-        # self.relu1d_1 = nn.ReLU(inplace=True)
-        #
-        # self.conv1d_m = nn.Conv2d(64,64,3,padding=1)###
-        # self.bn1d_m = nn.BatchNorm2d(64)
-        # self.relu1d_m = nn.ReLU(inplace=True)
-        #
-        # self.conv1d_2 = nn.Conv2d(64,64,3,padding=1)
-        # self.bn1d_2 = nn.BatchNorm2d(64)
-        # self.relu1d_2 = nn.ReLU(inplace=True)
+        self.conv1d_1 = nn.Conv2d(128,64,3,padding=1) # 256
+        self.bn1d_1 = nn.BatchNorm2d(64)
+        self.relu1d_1 = nn.ReLU(inplace=True)
+
+        self.conv1d_m = nn.Conv2d(64,64,3,padding=1)###
+        self.bn1d_m = nn.BatchNorm2d(64)
+        self.relu1d_m = nn.ReLU(inplace=True)
+
+        self.conv1d_2 = nn.Conv2d(64,64,3,padding=1)
+        self.bn1d_2 = nn.BatchNorm2d(64)
+        self.relu1d_2 = nn.ReLU(inplace=True)
 
         ## -------------Bilinear Upsampling--------------
-        self.upscore6 = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)###
-        self.upscore5 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)
-        self.upscore4 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)
-        self.upscore3 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)
-        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
+        self.upscore6 = nn.Upsample(scale_factor=32,mode='bilinear')###
+        self.upscore5 = nn.Upsample(scale_factor=16,mode='bilinear')
+        self.upscore4 = nn.Upsample(scale_factor=8,mode='bilinear')
+        self.upscore3 = nn.Upsample(scale_factor=4,mode='bilinear')
+        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear')
 
         ## -------------Side Output--------------
         self.outconvb = nn.Conv2d(512,1,3,padding=1)
@@ -249,10 +244,10 @@ class BASNet(nn.Module):
         self.outconv4 = nn.Conv2d(256,1,3,padding=1)
         self.outconv3 = nn.Conv2d(128,1,3,padding=1)
         self.outconv2 = nn.Conv2d(64,1,3,padding=1)
-        # self.outconv1 = nn.Conv2d(64,1,3,padding=1)
+        self.outconv1 = nn.Conv2d(64,1,3,padding=1)
 
         ## -------------Refine Module-------------
-        #self.refunet = RefUnet(1,64)
+        self.refunet = RefUnet(1,64)
 
 
     def forward(self,x):
@@ -260,91 +255,90 @@ class BASNet(nn.Module):
         hx = x
 
         ## -------------Encoder-------------
-        #hx = self.inconv(hx)
-        #hx = self.inbn(hx)
-        #hx = self.inrelu(hx)
+        hx = self.inconv(hx)
+        hx = self.inbn(hx)
+        hx = self.inrelu(hx)
+
+        h1 = self.encoder1(hx) # 256
+        h2 = self.encoder2(h1) # 128
+        h3 = self.encoder3(h2) # 64
+        h4 = self.encoder4(h3) # 32
 
-        h1 = self.encoder1(hx) # 128
-        h2 = self.encoder2(h1) # 64
-        h3 = self.encoder3(h2) # 32
-        h4 = self.encoder4(h3) # 16
-        h5 = self.encoder5(h4)  # 8
+        hx = self.pool4(h4) # 16
 
-        # hx = self.pool4(h4) # 16
-        #
-        # hx = self.resb5_1(hx)
-        # hx = self.resb5_2(hx)
-        # h5 = self.resb5_3(hx)
+        hx = self.resb5_1(hx)
+        hx = self.resb5_2(hx)
+        h5 = self.resb5_3(hx)
 
-        #hx = self.pool5(h5) # 8
+        hx = self.pool5(h5) # 8
 
-        #hx = self.resb6_1(hx)
-        #hx = self.resb6_2(hx)
-        #h6 = self.resb6_3(hx)
+        hx = self.resb6_1(hx)
+        hx = self.resb6_2(hx)
+        h6 = self.resb6_3(hx)
 
         ## -------------Bridge-------------
-        hx = self.relubg_1(self.bnbg_1(self.convbg_1(h5))) # 8
-        #hx = self.relubg_m(self.bnbg_m(self.convbg_m(hx)))
+        hx = self.relubg_1(self.bnbg_1(self.convbg_1(h6))) # 8
+        hx = self.relubg_m(self.bnbg_m(self.convbg_m(hx)))
         hbg = self.relubg_2(self.bnbg_2(self.convbg_2(hx)))
 
         ## -------------Decoder-------------
 
-        hx = self.relu6d_1(self.bn6d_1(self.conv6d_1(torch.cat((hbg,h5),1))))
-        #hx = self.relu6d_m(self.bn6d_m(self.conv6d_m(hx)))
-        hd5 = self.relu6d_2(self.bn6d_2(self.conv6d_2(hx)))
+        hx = self.relu6d_1(self.bn6d_1(self.conv6d_1(torch.cat((hbg,h6),1))))
+        hx = self.relu6d_m(self.bn6d_m(self.conv6d_m(hx)))
+        hd6 = self.relu6d_2(self.bn6d_2(self.conv6d_2(hx)))
 
-        hx = self.upscore2(hd5) # 8 -> 16
+        hx = self.upscore2(hd6) # 8 -> 16
 
-        hx = self.relu5d_1(self.bn5d_1(self.conv5d_1(torch.cat((hx,h4),1))))
-        #hx = self.relu5d_m(self.bn5d_m(self.conv5d_m(hx)))
-        hd4 = self.relu5d_2(self.bn5d_2(self.conv5d_2(hx)))
+        hx = self.relu5d_1(self.bn5d_1(self.conv5d_1(torch.cat((hx,h5),1))))
+        hx = self.relu5d_m(self.bn5d_m(self.conv5d_m(hx)))
+        hd5 = self.relu5d_2(self.bn5d_2(self.conv5d_2(hx)))
 
-        hx = self.upscore2(hd4) # 16 -> 32
+        hx = self.upscore2(hd5) # 16 -> 32
 
-        hx = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((hx,h3),1))))
-        #hx = self.relu4d_m(self.bn4d_m(self.conv4d_m(hx)))
-        hd3 = self.relu4d_2(self.bn4d_2(self.conv4d_2(hx)))
+        hx = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((hx,h4),1))))
+        hx = self.relu4d_m(self.bn4d_m(self.conv4d_m(hx)))
+        hd4 = self.relu4d_2(self.bn4d_2(self.conv4d_2(hx)))
 
-        hx = self.upscore2(hd3) # 32 -> 64
+        hx = self.upscore2(hd4) # 32 -> 64
 
-        hx = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((hx,h2),1))))
-        #hx = self.relu3d_m(self.bn3d_m(self.conv3d_m(hx)))
-        hd2 = self.relu3d_2(self.bn3d_2(self.conv3d_2(hx)))
+        hx = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((hx,h3),1))))
+        hx = self.relu3d_m(self.bn3d_m(self.conv3d_m(hx)))
+        hd3 = self.relu3d_2(self.bn3d_2(self.conv3d_2(hx)))
 
-        hx = self.upscore2(hd2) # 64 -> 128
+        hx = self.upscore2(hd3) # 64 -> 128
 
-        hx = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((hx,h1),1))))
-        #hx = self.relu2d_m(self.bn2d_m(self.conv2d_m(hx)))
-        hd1 = self.relu2d_2(self.bn2d_2(self.conv2d_2(hx)))
+        hx = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((hx,h2),1))))
+        hx = self.relu2d_m(self.bn2d_m(self.conv2d_m(hx)))
+        hd2 = self.relu2d_2(self.bn2d_2(self.conv2d_2(hx)))
 
-        # hx = self.upscore2(hd2) # 128 -> 256
-        #
-        # hx = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((hx,h1),1))))
-        # #hx = self.relu1d_m(self.bn1d_m(self.conv1d_m(hx)))
-        # hd1 = self.relu1d_2(self.bn1d_2(self.conv1d_2(hx)))
+        hx = self.upscore2(hd2) # 128 -> 256
+
+        hx = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((hx,h1),1))))
+        hx = self.relu1d_m(self.bn1d_m(self.conv1d_m(hx)))
+        hd1 = self.relu1d_2(self.bn1d_2(self.conv1d_2(hx)))
 
         ## -------------Side Output-------------
-        # db = self.outconvb(hbg)
-        # db = self.upscore6(db) # 8->256
-        #
-        # # d6 = self.outconv6(hd5)
-        # # d6 = self.upscore6(d6) # 8->256
-        #
-        # d5 = self.outconv5(hd5)
-        # d5 = self.upscore6(d5) # 8->256
-        #
-        # d4 = self.outconv5(hd4)
-        # d4 = self.upscore5(d4) # 16->256
-        #
-        # d3 = self.outconv4(hd3)
-        # d3 = self.upscore4(d3) # 32->256
-        #
-        # d2 = self.outconv3(hd2)
-        # d2 = self.upscore3(d2) # 64->256
-
-        d1 = self.outconv2(hd1) # 128 -> 256
-        d1 = self.upscore2(d1)
+        db = self.outconvb(hbg)
+        db = self.upscore6(db) # 8->256
+
+        d6 = self.outconv6(hd6)
+        d6 = self.upscore6(d6) # 8->256
+
+        d5 = self.outconv5(hd5)
+        d5 = self.upscore5(d5) # 16->256
+
+        d4 = self.outconv4(hd4)
+        d4 = self.upscore4(d4) # 32->256
+
+        d3 = self.outconv3(hd3)
+        d3 = self.upscore3(d3) # 64->256
+
+        d2 = self.outconv2(hd2)
+        d2 = self.upscore2(d2) # 128->256
+
+        d1 = self.outconv1(hd1) # 256
+
         ## -------------Refine Module-------------
-        #dout = self.refunet(d1) # 256
+        dout = self.refunet(d1) # 256
 
-        return F.sigmoid(d1)#, F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(db)
+        return F.sigmoid(dout), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6), F.sigmoid(db)
\ No newline at end of file
diff --git a/model/BASNet_mobile.py b/model/BASNet_mobile.py
new file mode 100755
index 0000000..a2155e3
--- /dev/null
+++ b/model/BASNet_mobile.py
@@ -0,0 +1,350 @@
+import torch
+import torch.nn as nn
+from torchvision import models
+import torch.nn.functional as F
+
+from .resnet_model import *
+from torchkeras import summary
+
+
+class RefUnet(nn.Module):
+    def __init__(self,in_ch,inc_ch):
+        super(RefUnet, self).__init__()
+
+        self.conv0 = nn.Conv2d(in_ch,inc_ch,3,padding=1)
+
+        self.conv1 = nn.Conv2d(inc_ch,64,3,padding=1)
+        self.bn1 = nn.BatchNorm2d(64)
+        self.relu1 = nn.ReLU(inplace=True)
+
+        self.pool1 = nn.MaxPool2d(2,2,ceil_mode=True)
+
+        self.conv2 = nn.Conv2d(64,64,3,padding=1)
+        self.bn2 = nn.BatchNorm2d(64)
+        self.relu2 = nn.ReLU(inplace=True)
+
+        self.pool2 = nn.MaxPool2d(2,2,ceil_mode=True)
+
+        self.conv3 = nn.Conv2d(64,64,3,padding=1)
+        self.bn3 = nn.BatchNorm2d(64)
+        self.relu3 = nn.ReLU(inplace=True)
+
+        self.pool3 = nn.MaxPool2d(2,2,ceil_mode=True)
+
+        self.conv4 = nn.Conv2d(64,64,3,padding=1)
+        self.bn4 = nn.BatchNorm2d(64)
+        self.relu4 = nn.ReLU(inplace=True)
+
+        self.pool4 = nn.MaxPool2d(2,2,ceil_mode=True)
+
+        #####
+
+        self.conv5 = nn.Conv2d(64,64,3,padding=1)
+        self.bn5 = nn.BatchNorm2d(64)
+        self.relu5 = nn.ReLU(inplace=True)
+
+        #####
+
+        self.conv_d4 = nn.Conv2d(128,64,3,padding=1)
+        self.bn_d4 = nn.BatchNorm2d(64)
+        self.relu_d4 = nn.ReLU(inplace=True)
+
+        self.conv_d3 = nn.Conv2d(128,64,3,padding=1)
+        self.bn_d3 = nn.BatchNorm2d(64)
+        self.relu_d3 = nn.ReLU(inplace=True)
+
+        self.conv_d2 = nn.Conv2d(128,64,3,padding=1)
+        self.bn_d2 = nn.BatchNorm2d(64)
+        self.relu_d2 = nn.ReLU(inplace=True)
+
+        self.conv_d1 = nn.Conv2d(128,64,3,padding=1)
+        self.bn_d1 = nn.BatchNorm2d(64)
+        self.relu_d1 = nn.ReLU(inplace=True)
+
+        self.conv_d0 = nn.Conv2d(64,1,3,padding=1)
+
+        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear')
+
+
+    def forward(self,x):
+
+        hx = x
+        hx = self.conv0(hx)
+
+        hx1 = self.relu1(self.bn1(self.conv1(hx)))
+        hx = self.pool1(hx1)
+
+        hx2 = self.relu2(self.bn2(self.conv2(hx)))
+        hx = self.pool2(hx2)
+
+        hx3 = self.relu3(self.bn3(self.conv3(hx)))
+        hx = self.pool3(hx3)
+
+        hx4 = self.relu4(self.bn4(self.conv4(hx)))
+        hx = self.pool4(hx4)
+
+        hx5 = self.relu5(self.bn5(self.conv5(hx)))
+
+        hx = self.upscore2(hx5)
+
+        d4 = self.relu_d4(self.bn_d4(self.conv_d4(torch.cat((hx,hx4),1))))
+        hx = self.upscore2(d4)
+
+        d3 = self.relu_d3(self.bn_d3(self.conv_d3(torch.cat((hx,hx3),1))))
+        hx = self.upscore2(d3)
+
+        d2 = self.relu_d2(self.bn_d2(self.conv_d2(torch.cat((hx,hx2),1))))
+        hx = self.upscore2(d2)
+
+        d1 = self.relu_d1(self.bn_d1(self.conv_d1(torch.cat((hx,hx1),1))))
+
+        residual = self.conv_d0(d1)
+
+        return x + residual
+
+class BASNet(nn.Module):
+    def __init__(self,n_channels,n_classes):
+        super(BASNet,self).__init__()
+
+        resnet = models.mobilenet_v2(pretrained=True)
+        #print(summary(resnet, input_shape=(3, 224, 224)))
+        #print(resnet)
+
+        ## -------------Encoder--------------
+
+        #self.inconv = nn.Conv2d(n_channels,32,3,padding=1)
+        #self.inbn = nn.BatchNorm2d(32)
+        #self.inrelu = nn.ReLU(inplace=True)
+
+        #stage 1
+        self.encoder1 = resnet.features[:2] #112
+        #stage 2
+        self.encoder2 = resnet.features[2:4] #56
+        #stage 3
+        self.encoder3 = resnet.features[4:7] #28
+        #stage 4
+        self.encoder4 = resnet.features[7:14] #14
+        #stage 5
+        self.encoder5 = resnet.features[14:18] #7
+
+        # self.pool4 = nn.MaxPool2d(2,2,ceil_mode=True)
+        #
+        # #stage 5
+        # self.resb5_1 = BasicBlock(512,512)
+        # self.resb5_2 = BasicBlock(512,512)
+        # self.resb5_3 = BasicBlock(512,512) #14
+        #
+        # self.pool5 = nn.MaxPool2d(2,2,ceil_mode=True)
+        #
+        # #stage 6
+        # self.resb6_1 = BasicBlock(512,512)
+        # self.resb6_2 = BasicBlock(512,512)
+        # self.resb6_3 = BasicBlock(512,512) #7
+
+        ## -------------Bridge--------------
+
+        #stage Bridge
+        self.convbg_1 = nn.Conv2d(320,512,3,dilation=2, padding=2) # 7
+        self.bnbg_1 = nn.BatchNorm2d(512)
+        self.relubg_1 = nn.ReLU(inplace=True)
+        self.convbg_m = nn.Conv2d(512,512,3,dilation=2, padding=2)
+        self.bnbg_m = nn.BatchNorm2d(512)
+        self.relubg_m = nn.ReLU(inplace=True)
+        self.convbg_2 = nn.Conv2d(512,512,3,dilation=2, padding=2)
+        self.bnbg_2 = nn.BatchNorm2d(512)
+        self.relubg_2 = nn.ReLU(inplace=True)
+
+        ## -------------Decoder--------------
+
+        #stage 6d
+        self.conv6d_1 = nn.Conv2d(832,512,3,padding=1) # 16
+        self.bn6d_1 = nn.BatchNorm2d(512)
+        self.relu6d_1 = nn.ReLU(inplace=True)
+
+        self.conv6d_m = nn.Conv2d(512,512,3,dilation=2, padding=2)###
+        self.bn6d_m = nn.BatchNorm2d(512)
+        self.relu6d_m = nn.ReLU(inplace=True)
+
+        self.conv6d_2 = nn.Conv2d(512,512,3,dilation=2, padding=2)
+        self.bn6d_2 = nn.BatchNorm2d(512)
+        self.relu6d_2 = nn.ReLU(inplace=True)
+
+        #stage 5d
+        self.conv5d_1 = nn.Conv2d(608,512,3,padding=1) # 16
+        self.bn5d_1 = nn.BatchNorm2d(512)
+        self.relu5d_1 = nn.ReLU(inplace=True)
+
+        self.conv5d_m = nn.Conv2d(512,512,3,padding=1)###
+        self.bn5d_m = nn.BatchNorm2d(512)
+        self.relu5d_m = nn.ReLU(inplace=True)
+
+        self.conv5d_2 = nn.Conv2d(512,512,3,padding=1)
+        self.bn5d_2 = nn.BatchNorm2d(512)
+        self.relu5d_2 = nn.ReLU(inplace=True)
+
+        #stage 4d
+        self.conv4d_1 = nn.Conv2d(544,512,3,padding=1) # 32
+        self.bn4d_1 = nn.BatchNorm2d(512)
+        self.relu4d_1 = nn.ReLU(inplace=True)
+
+        self.conv4d_m = nn.Conv2d(512,512,3,padding=1)###
+        self.bn4d_m = nn.BatchNorm2d(512)
+        self.relu4d_m = nn.ReLU(inplace=True)
+
+        self.conv4d_2 = nn.Conv2d(512,256,3,padding=1)
+        self.bn4d_2 = nn.BatchNorm2d(256)
+        self.relu4d_2 = nn.ReLU(inplace=True)
+
+        #stage 3d
+        self.conv3d_1 = nn.Conv2d(280,256,3,padding=1) # 64
+        self.bn3d_1 = nn.BatchNorm2d(256)
+        self.relu3d_1 = nn.ReLU(inplace=True)
+
+        self.conv3d_m = nn.Conv2d(256,256,3,padding=1)###
+        self.bn3d_m = nn.BatchNorm2d(256)
+        self.relu3d_m = nn.ReLU(inplace=True)
+
+        self.conv3d_2 = nn.Conv2d(256,128,3,padding=1)
+        self.bn3d_2 = nn.BatchNorm2d(128)
+        self.relu3d_2 = nn.ReLU(inplace=True)
+
+        #stage 2d
+
+        self.conv2d_1 = nn.Conv2d(144,128,3,padding=1) # 128
+        self.bn2d_1 = nn.BatchNorm2d(128)
+        self.relu2d_1 = nn.ReLU(inplace=True)
+
+        self.conv2d_m = nn.Conv2d(128,128,3,padding=1)###
+        self.bn2d_m = nn.BatchNorm2d(128)
+        self.relu2d_m = nn.ReLU(inplace=True)
+
+        self.conv2d_2 = nn.Conv2d(128,64,3,padding=1)
+        self.bn2d_2 = nn.BatchNorm2d(64)
+        self.relu2d_2 = nn.ReLU(inplace=True)
+
+        #stage 1d
+        # self.conv1d_1 = nn.Conv2d(128,64,3,padding=1) # 256
+        # self.bn1d_1 = nn.BatchNorm2d(64)
+        # self.relu1d_1 = nn.ReLU(inplace=True)
+        #
+        # self.conv1d_m = nn.Conv2d(64,64,3,padding=1)###
+        # self.bn1d_m = nn.BatchNorm2d(64)
+        # self.relu1d_m = nn.ReLU(inplace=True)
+        #
+        # self.conv1d_2 = nn.Conv2d(64,64,3,padding=1)
+        # self.bn1d_2 = nn.BatchNorm2d(64)
+        # self.relu1d_2 = nn.ReLU(inplace=True)
+
+        ## -------------Bilinear Upsampling--------------
+        self.upscore6 = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True)###
+        self.upscore5 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)
+        self.upscore4 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)
+        self.upscore3 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)
+        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
+
+        ## -------------Side Output--------------
+        self.outconvb = nn.Conv2d(512,1,3,padding=1)
+        self.outconv6 = nn.Conv2d(512,1,3,padding=1)
+        self.outconv5 = nn.Conv2d(512,1,3,padding=1)
+        self.outconv4 = nn.Conv2d(256,1,3,padding=1)
+        self.outconv3 = nn.Conv2d(128,1,3,padding=1)
+        self.outconv2 = nn.Conv2d(64,1,3,padding=1)
+        # self.outconv1 = nn.Conv2d(64,1,3,padding=1)
+
+        ## -------------Refine Module-------------
+        #self.refunet = RefUnet(1,64)
+
+
+    def forward(self,x):
+
+        hx = x
+
+        ## -------------Encoder-------------
+        #hx = self.inconv(hx)
+        #hx = self.inbn(hx)
+        #hx = self.inrelu(hx)
+
+        h1 = self.encoder1(hx) # 128
+        h2 = self.encoder2(h1) # 64
+        h3 = self.encoder3(h2) # 32
+        h4 = self.encoder4(h3) # 16
+        h5 = self.encoder5(h4)  # 8
+
+        # hx = self.pool4(h4) # 16
+        #
+        # hx = self.resb5_1(hx)
+        # hx = self.resb5_2(hx)
+        # h5 = self.resb5_3(hx)
+
+        #hx = self.pool5(h5) # 8
+
+        #hx = self.resb6_1(hx)
+        #hx = self.resb6_2(hx)
+        #h6 = self.resb6_3(hx)
+
+        ## -------------Bridge-------------
+        hx = self.relubg_1(self.bnbg_1(self.convbg_1(h5))) # 8
+        #hx = self.relubg_m(self.bnbg_m(self.convbg_m(hx)))
+        hbg = self.relubg_2(self.bnbg_2(self.convbg_2(hx)))
+
+        ## -------------Decoder-------------
+
+        hx = self.relu6d_1(self.bn6d_1(self.conv6d_1(torch.cat((hbg,h5),1))))
+        #hx = self.relu6d_m(self.bn6d_m(self.conv6d_m(hx)))
+        hd5 = self.relu6d_2(self.bn6d_2(self.conv6d_2(hx)))
+
+        hx = self.upscore2(hd5) # 8 -> 16
+
+        hx = self.relu5d_1(self.bn5d_1(self.conv5d_1(torch.cat((hx,h4),1))))
+        #hx = self.relu5d_m(self.bn5d_m(self.conv5d_m(hx)))
+        hd4 = self.relu5d_2(self.bn5d_2(self.conv5d_2(hx)))
+
+        hx = self.upscore2(hd4) # 16 -> 32
+
+        hx = self.relu4d_1(self.bn4d_1(self.conv4d_1(torch.cat((hx,h3),1))))
+        #hx = self.relu4d_m(self.bn4d_m(self.conv4d_m(hx)))
+        hd3 = self.relu4d_2(self.bn4d_2(self.conv4d_2(hx)))
+
+        hx = self.upscore2(hd3) # 32 -> 64
+
+        hx = self.relu3d_1(self.bn3d_1(self.conv3d_1(torch.cat((hx,h2),1))))
+        #hx = self.relu3d_m(self.bn3d_m(self.conv3d_m(hx)))
+        hd2 = self.relu3d_2(self.bn3d_2(self.conv3d_2(hx)))
+
+        hx = self.upscore2(hd2) # 64 -> 128
+
+        hx = self.relu2d_1(self.bn2d_1(self.conv2d_1(torch.cat((hx,h1),1))))
+        #hx = self.relu2d_m(self.bn2d_m(self.conv2d_m(hx)))
+        hd1 = self.relu2d_2(self.bn2d_2(self.conv2d_2(hx)))
+
+        # hx = self.upscore2(hd2) # 128 -> 256
+        #
+        # hx = self.relu1d_1(self.bn1d_1(self.conv1d_1(torch.cat((hx,h1),1))))
+        # #hx = self.relu1d_m(self.bn1d_m(self.conv1d_m(hx)))
+        # hd1 = self.relu1d_2(self.bn1d_2(self.conv1d_2(hx)))
+
+        ## -------------Side Output-------------
+        # db = self.outconvb(hbg)
+        # db = self.upscore6(db) # 8->256
+        #
+        # # d6 = self.outconv6(hd5)
+        # # d6 = self.upscore6(d6) # 8->256
+        #
+        # d5 = self.outconv5(hd5)
+        # d5 = self.upscore6(d5) # 8->256
+        #
+        # d4 = self.outconv5(hd4)
+        # d4 = self.upscore5(d4) # 16->256
+        #
+        # d3 = self.outconv4(hd3)
+        # d3 = self.upscore4(d3) # 32->256
+        #
+        # d2 = self.outconv3(hd2)
+        # d2 = self.upscore3(d2) # 64->256
+
+        d1 = self.outconv2(hd1) # 128 -> 256
+        d1 = self.upscore2(d1)
+        ## -------------Refine Module-------------
+        #dout = self.refunet(d1) # 256
+
+        return F.sigmoid(d1)#, F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(db)
diff --git a/model/__pycache__/BASNet.cpython-36.pyc b/model/__pycache__/BASNet.cpython-36.pyc
deleted file mode 100644
index c6ce0e0..0000000
Binary files a/model/__pycache__/BASNet.cpython-36.pyc and /dev/null differ
diff --git a/model/resnet_model.py b/model/resnet_model.py
index 5d7d82e..65b539a 100644
--- a/model/resnet_model.py
+++ b/model/resnet_model.py
@@ -4,7 +4,7 @@ import math
 import torch.utils.model_zoo as model_zoo
 import torch
 import torchvision
-
+#
 # __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
 #            'resnet152', 'ResNet34P','ResNet50S','ResNet50P','ResNet101P']
 #
diff --git a/rt_test.py b/rt_test.py
new file mode 100644
index 0000000..3ac2bbe
--- /dev/null
+++ b/rt_test.py
@@ -0,0 +1,196 @@
+import pycuda.driver as cuda
+import pycuda.autoinit
+import numpy as np
+import tensorrt as trt
+import cv2
+import torch
+import time
+from albumentations import Resize, Compose
+from albumentations.pytorch.transforms import ToTensorV2
+from albumentations.augmentations.transforms import Normalize
+from data import test_dataset
+from skimage import io, transform
+from torch.autograd import Variable
+# logger to capture errors, warnings, and other information during the build and inference phases
+
+
+TRT_LOGGER = trt.Logger()
+
+
+def normPRED(d):
+	ma = torch.max(d)
+	mi = torch.min(d)
+	dn = (d-mi)/(ma-mi)
+	return dn
+
+
+def preprocess_image(img_path):
+    transforms = Compose([
+        Resize(224, 224, interpolation=cv2.INTER_NEAREST),
+        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
+        ToTensorV2(),
+    ])
+
+    # read input image
+    input_img = cv2.imread(img_path)
+    # do transformations
+    input_data = transforms(image=input_img)["image"]
+    batch_data = torch.unsqueeze(input_data, 0)
+    return batch_data
+
+def postprocess(output_data):
+    # get class names
+    with open("imagenet_classes.txt") as f:
+        classes = [line.strip() for line in f.readlines()]
+    # calculate human-readable value by softmax
+    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100
+    # find top predicted classes
+    _, indices = torch.sort(output_data, descending=True)
+    i = 0
+    # print the top classes predicted by the model
+    while confidences[indices[0][i]] > 0.5:
+        class_idx = indices[0][i]
+        print(
+            "class:",
+            classes[class_idx],
+            ", confidence:",
+            confidences[class_idx].item(),
+            "%, index:",
+            class_idx.item(),
+        )
+        i += 1
+
+
+def save_output(image_name, pred, d_dir, o_dir):
+	pred = pred.squeeze()
+	pred = pred.cpu().data.numpy()
+	th = 0.2
+	pred[pred > th] = 1
+	pred[pred <= th] = 0
+
+	img_name = image_name.split("/")[-1]
+	image = io.imread(image_name)
+
+	mask = transform.resize(pred, (image.shape[0],image.shape[1]), anti_aliasing=False, mode = 'constant', order=0)
+	mask = np.tile(np.expand_dims(mask, axis=-1), (1, 1, 3))
+	#kernel = np.ones((3, 3), np.uint8)
+	#mask = cv2.erode(mask, kernel, iterations=4)
+	olay = image * mask
+
+
+	#pb_np = np.array(imo)
+
+	aaa = img_name.split(".")
+	bbb = aaa[0:-1]
+	imidx = bbb[0]
+	for i in range(1,len(bbb)):
+		imidx = imidx + "." + bbb[i]
+
+	io.imsave(o_dir+imidx+'.jpg', olay)
+	io.imsave(d_dir + imidx + '.jpg', mask)
+
+
+def build_engine(onnx_file_path):
+
+    # initialize TensorRT engine and parse ONNX model
+    builder = trt.Builder(TRT_LOGGER)
+    network = builder.create_network(1)
+    parser = trt.OnnxParser(network, TRT_LOGGER)
+    # parse ONNX
+    with open(onnx_file_path, 'rb') as model:
+        print('Beginning ONNX file parsing')
+        parser.parse(model.read())
+    print('Completed parsing of ONNX file')
+
+    # allow TensorRT to use up to 1GB of GPU memory for tactic selection
+    builder.max_workspace_size = 16 << 30
+    # we have only one image in batch
+    builder.max_batch_size = 4
+    # use FP16 mode if possible
+    if builder.platform_has_fast_fp16:
+        builder.fp16_mode = True
+
+    # generate TensorRT engine optimized for the target platform
+    print('Building an engine...')
+    engine = builder.build_cuda_engine(network)
+    #context = engine.create_execution_context()
+    print("Completed creating Engine")
+    return engine
+
+
+def main(makeengine=False):
+    if makeengine:
+        # initialize TensorRT engine and parse ONNX model
+        engine = build_engine('basnet.onnx')
+        serialized_engine = engine.serialize()
+        with open('basnet.engine', 'wb') as f:
+            f.write(serialized_engine)
+
+    with open('basnet.engine', 'rb') as f, trt.Runtime(TRT_LOGGER) as runtime:
+        engine = runtime.deserialize_cuda_engine(f.read())
+    context = engine.create_execution_context()
+    # get sizes of input and output and allocate memory required for input data and for output data
+    for binding in engine:
+        if engine.binding_is_input(binding):  # we expect only one input
+            input_shape = engine.get_binding_shape(binding)
+            input_size = trt.volume(input_shape) * engine.max_batch_size * np.dtype(np.float32).itemsize  # in bytes
+            device_input = cuda.mem_alloc(input_size)
+        else:  # and one output
+            output_shape = engine.get_binding_shape(binding)
+            # create page-locked memory buffers (i.e. won't be swapped to disk)
+            host_output = cuda.pagelocked_empty(trt.volume(output_shape) * engine.max_batch_size, dtype=np.float32)
+            device_output = cuda.mem_alloc(host_output.nbytes)
+        # Create a stream in which to copy inputs/outputs and run inference.
+    stream = cuda.Stream()
+    # preprocess input data
+    #host_input = np.array(preprocess_image("ILSVRC2012_test_00000232.jpg").numpy(), dtype=np.float32, order='C')
+    # for i in range(10):
+    #     cuda.memcpy_htod_async(device_input, host_input, stream)
+    #     # run inference
+    #     start = time.time()
+    #     context.execute_async(bindings=[int(device_input), int(device_output)], stream_handle=stream.handle)
+    #     cuda.memcpy_dtoh_async(host_output, device_output, stream)
+    #     stream.synchronize()
+    #     print(time.time()-start)
+    #     # postprocess results
+    #     print(host_output.shape, output_shape)
+    #     output_data = torch.Tensor(host_output).reshape(output_shape[2], output_shape[3])
+    #     print(output_data.shape)
+    #     pred = normPRED(output_data)
+    #     print(pred)
+
+    image_dir = '/home/hypevr/Desktop/data_0616/xy/other/image/'  # '/media/hypevr/KEY/tonaci_selected/'#'./test_data/test_images/'
+    prediction_dir = '/home/hypevr/Desktop/data_0616/xy/other/mask/'  # '/media/hypev/KEY/tonaci_selected_masks/'
+    olay_dir = '/home/hypevr/Desktop/data_0616/xy/other/olay/'  # '/media/hypevr/KEY/tonaci_selected_olay/'
+    model_dir = './saved_models/basnet_bsi_human2_fr0.2_pb_0.2/basnet_213.pth'  # refine/
+    plate_dir = '/home/hypevr/Desktop/data_0616/xy/3/back'
+    test_loader = test_dataset(image_dir, image_dir, 320, True)
+    for i in range(test_loader.size):
+        image_orig, host_input, gt, name = test_loader.load_data()
+        host_input = host_input.numpy()
+        print(host_input.shape)
+        host_input = np.tile(host_input, (4, 1, 1, 1))
+        #host_input = host_input.transpose((0, 3, 1, 2))
+        host_input = np.array(host_input, order='C')
+        print(host_input.shape)
+        #host_input = Variable(host_input)
+        #print(host_input)
+        ##inputs_test = data_test[0]
+        cuda.memcpy_htod_async(device_input, host_input, stream)
+
+        #inputs_test = inputs_test.type(torch.FloatTensor)
+        start = time.time()
+        context.execute_async(bindings=[int(device_input), int(device_output)], stream_handle=stream.handle)
+        cuda.memcpy_dtoh_async(host_output, device_output, stream)
+        stream.synchronize()
+        print(time.time() - start)
+        output_data = torch.Tensor(host_output).reshape(output_shape[0], output_shape[1], output_shape[2], output_shape[3])
+        #pred = normPRED(output_data)
+        # pred = overlay(image_resized, pred.squeeze().cpu().data.numpy())
+        # save results to test_results folder
+        #save_output(image_dir + name, pred, prediction_dir, olay_dir)
+
+
+if __name__ == "__main__":
+    # execute only if run as a script
+    main(False)
diff --git a/temp.jpg b/temp.jpg
deleted file mode 100644
index 6456626..0000000
Binary files a/temp.jpg and /dev/null differ
diff --git a/test_data/test_results/0003.png b/test_data/test_results/0003.png
deleted file mode 100644
index f7bd90c..0000000
Binary files a/test_data/test_results/0003.png and /dev/null differ
diff --git a/test_data/test_results/0005.png b/test_data/test_results/0005.png
deleted file mode 100644
index ad627ef..0000000
Binary files a/test_data/test_results/0005.png and /dev/null differ
diff --git a/test_data/test_results/0010.png b/test_data/test_results/0010.png
deleted file mode 100644
index babe1b8..0000000
Binary files a/test_data/test_results/0010.png and /dev/null differ
diff --git a/test_data/test_results/0012.png b/test_data/test_results/0012.png
deleted file mode 100644
index 55ba083..0000000
Binary files a/test_data/test_results/0012.png and /dev/null differ
